# -*- coding: utf-8 -*-
"""Modelo Unidadesv2 - Improved Version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ow02mmgGkvoWee-NNwfXT4Km-sYFW4WF

##**Inicial**
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import os
import pickle
import pandas as pd
import seaborn as sns

from catboost import CatBoostRegressor, Pool
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')

# Prepare Data - IMPROVED VERSION

def prepare_final_dataset_improved(df, meses_futuros=None):
    """
    Improved data preparation with better feature engineering and validation
    """
    df = df.copy()

    # 1. BASIC CLEANING
    df.rename(columns={
        'NombreTPV': 'Tienda',
        'Fecha Documento': 'ds'
    }, inplace=True)

    # Ensure numeric columns are properly converted
    numeric_cols = ['Cantidad', 'P.V.P.', 'Subtotal', 'Precio Coste']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

    # Handle season data
    df['Temporada'] = df['Temporada'].fillna('Unknown_Season').astype(str)
    df['Season'] = df['Temporada'].str[0].str.upper()

    # 2. IMPROVED SEASONALITY DEFINITION
    # Use more sophisticated seasonality detection
    df['Cantidad_fuera_temporada'] = np.where(df['vendido_fuera_temporada'] == 1, df['Cantidad'], 0)
    df['Cantidad_en_temporada'] = np.where(df['vendido_fuera_temporada'] == 0, df['Cantidad'], 0)

    # 3. MONTHLY AGGREGATION WITH BETTER GROUPING
    monthly = df.groupby(
        ['Tienda', 'Descripci√≥n Familia', 'Talla', 'Season', pd.Grouper(key='ds', freq='MS')]
    ).agg({
        'Cantidad': 'sum',
        'Cantidad_en_temporada': 'sum',
        'Cantidad_fuera_temporada': 'sum',
        'P.V.P.': 'mean',
        'Precio Coste': 'mean',
        'Subtotal': 'sum',
    }).reset_index()

    # Ensure categorical columns are strings
    categorical_cols = ['Tienda', 'Descripci√≥n Familia', 'Talla', 'Season']
    for col in categorical_cols:
        monthly[col] = monthly[col].astype(str)

    # 4. ADD FUTURE MONTHS IF SPECIFIED
    if meses_futuros is not None and len(meses_futuros) > 0:
        combinaciones = monthly[['Tienda', 'Descripci√≥n Familia', 'Talla', 'Season']].drop_duplicates()
        meses_futuros = pd.to_datetime(meses_futuros)

        futuras = []
        for mes in meses_futuros:
            temp = combinaciones.copy()
            temp['ds'] = mes
            futuras.append(temp)

        futuras_df = pd.concat(futuras, ignore_index=True)
        monthly = pd.concat([monthly, futuras_df], ignore_index=True)
        monthly = monthly.sort_values(['Tienda', 'Descripci√≥n Familia', 'Season', 'ds']).reset_index(drop=True)

    # 5. IMPROVED TEMPORAL FEATURES
    monthly['mes'] = monthly['ds'].dt.month
    monthly['a√±o'] = monthly['ds'].dt.year
    monthly['mes_cat'] = monthly['mes'].astype(str).str.zfill(2)
    monthly['mes_ano'] = monthly['ds'].dt.strftime('%Y-%m')
    monthly['quarter'] = ((monthly['mes'] - 1) // 3) + 1
    monthly['quarter'] = monthly['quarter'].astype(str)

    # 6. IMPROVED LAGS AND ROLLING FEATURES
    group_cols = ['Tienda', 'Descripci√≥n Familia', 'Season']

    # More comprehensive lag features
    for lag in [1, 2, 3, 6, 12, 24]:
        monthly[f'lag_{lag}_cantidad_en_temporada'] = monthly.groupby(group_cols)['Cantidad_en_temporada'].shift(lag)
        monthly[f'lag_{lag}_cantidad_fuera_temporada'] = monthly.groupby(group_cols)['Cantidad_fuera_temporada'].shift(lag)

    # Rolling features with different windows
    for window in [3, 6, 12]:
        monthly[f'rolling_{window}m_cantidad_en_temporada'] = monthly.groupby(group_cols)['Cantidad_en_temporada'].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).mean()
        )
        monthly[f'rolling_{window}m_cantidad_fuera_temporada'] = monthly.groupby(group_cols)['Cantidad_fuera_temporada'].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).mean()
        )

    # 7. IMPROVED SEASONALITY FEATURES
    monthly['month_sin'] = np.sin(2 * np.pi * monthly['mes'] / 12)
    monthly['month_cos'] = np.cos(2 * np.pi * monthly['mes'] / 12)
    monthly['quarter_sin'] = np.sin(2 * np.pi * monthly['quarter'].astype(int) / 4)
    monthly['quarter_cos'] = np.cos(2 * np.pi * monthly['quarter'].astype(int) / 4)

    # 8. IMPROVED BUSINESS FEATURES
    monthly['margen_unitario'] = monthly['P.V.P.'] - monthly['Precio Coste']
    monthly['precio_relativo'] = monthly['P.V.P.'] / (monthly['Precio Coste'] + 1e-5)
    monthly['es_rebajas'] = monthly['mes'].isin([2, 7]).astype(int)
    monthly['es_verano'] = (monthly['Season'] == 'V').astype(int)
    monthly['es_invierno'] = (monthly['Season'] == 'I').astype(int)

    # 9. IMPROVED SEASONALITY INDEX
    seasonality_index = monthly.groupby('Descripci√≥n Familia').agg({
        'es_verano': 'mean',
        'es_invierno': 'mean'
    }).reset_index()
    seasonality_index.columns = ['Descripci√≥n Familia', 'seasonality_index_verano', 'seasonality_index_invierno']
    monthly = monthly.merge(seasonality_index, on='Descripci√≥n Familia', how='left')

    # 10. IMPROVED TREND FEATURES
    monthly['tendencia_en_temporada'] = monthly['lag_1_cantidad_en_temporada'] - monthly['lag_3_cantidad_en_temporada']
    monthly['tendencia_3m_vs_12m_en_temporada'] = monthly['rolling_3m_cantidad_en_temporada'] / (monthly['lag_12_cantidad_en_temporada'] + 1e-5)
    monthly['tendencia_6m_vs_12m_en_temporada'] = monthly['rolling_6m_cantidad_en_temporada'] / (monthly['lag_12_cantidad_en_temporada'] + 1e-5)

    monthly['tendencia_fuera_temporada'] = monthly['lag_1_cantidad_fuera_temporada'] - monthly['lag_3_cantidad_fuera_temporada']
    monthly['tendencia_3m_vs_12m_fuera_temporada'] = monthly['rolling_3m_cantidad_fuera_temporada'] / (monthly['lag_12_cantidad_fuera_temporada'] + 1e-5)
    monthly['tendencia_6m_vs_12m_fuera_temporada'] = monthly['rolling_6m_cantidad_fuera_temporada'] / (monthly['lag_12_cantidad_fuera_temporada'] + 1e-5)

    # 11. IMPROVED ACCUMULATIVE FEATURES
    monthly['acumulado_anterior'] = monthly.groupby(group_cols)['Cantidad'].cumsum().shift(1)
    monthly['cantidad_yoy'] = monthly.groupby(group_cols)['Cantidad'].shift(12)
    monthly['delta_yoy'] = (monthly['Cantidad'] - monthly['cantidad_yoy']) / (monthly['cantidad_yoy'] + 1e-5)

    # 12. VOLATILITY FEATURES
    monthly['volatilidad_en_temporada'] = monthly.groupby(group_cols)['Cantidad_en_temporada'].transform(
        lambda x: x.rolling(6, min_periods=1).std()
    )
    monthly['volatilidad_fuera_temporada'] = monthly.groupby(group_cols)['Cantidad_fuera_temporada'].transform(
        lambda x: x.rolling(6, min_periods=1).std()
    )

    # 13. IMPROVED DATASET SEPARATION
    # More robust filtering for training data
    en_temporada = monthly[
        (monthly['Cantidad_en_temporada'] > 0) &
        monthly[['lag_1_cantidad_en_temporada', 'lag_3_cantidad_en_temporada', 'lag_12_cantidad_en_temporada']].notna().all(axis=1)
    ].copy()

    fuera_temporada = monthly[
        (monthly['Cantidad_fuera_temporada'] > 0) &
        monthly[['lag_1_cantidad_fuera_temporada', 'lag_3_cantidad_fuera_temporada', 'lag_12_cantidad_fuera_temporada']].notna().all(axis=1)
    ].copy()

    return en_temporada, fuera_temporada, monthly

def train_best_model_improved(train_val_df, features, categorical_features, target_col, best_params):
    """
    Improved model training with better validation and error handling
    """
    # Ensure all features exist
    missing_features = [f for f in features if f not in train_val_df.columns]
    if missing_features:
        print(f"‚ö†Ô∏è Missing features: {missing_features}")
        features = [f for f in features if f in train_val_df.columns]
    
    # Convert categorical features to indices
    cat_feature_indices = [train_val_df[features].columns.get_loc(c) for c in categorical_features if c in features]
    
    # Prepare data
    X = train_val_df[features].copy()
    y = train_val_df[target_col].copy()
    
    # Handle missing values
    X = X.fillna(0)
    y = y.fillna(0)
    
    # Ensure categorical features are strings
    for col in categorical_features:
        if col in X.columns:
            X[col] = X[col].astype(str)
    
    # Create pool
    pool = Pool(data=X, label=y, cat_features=cat_feature_indices)
    
    # Train model with early stopping
    model = CatBoostRegressor(**best_params)
    model.fit(pool, verbose=100)
    
    print(f"‚úÖ Model trained successfully with {len(features)} features")
    return model

# IMPROVED HYPERPARAMETERS
best_params_en_improved = {
    'iterations': 1500,
    'learning_rate': 0.01,
    'depth': 8,
    'l2_leaf_reg': 3,
    'loss_function': 'RMSE',
    'random_seed': 42,
    'verbose': 100,
    'eval_metric': 'RMSE',
    'bagging_temperature': 0.8,
    'subsample': 0.8,
    'early_stopping_rounds': 100,
    'grow_policy': 'Lossguide',
    'min_data_in_leaf': 10,
    'max_leaves': 64
}

best_params_fuera_improved = {
    'iterations': 1500,
    'learning_rate': 0.01,
    'depth': 8,
    'l2_leaf_reg': 3,
    'loss_function': 'RMSE',
    'random_seed': 42,
    'verbose': 100,
    'eval_metric': 'RMSE',
    'bagging_temperature': 0.8,
    'subsample': 0.8,
    'early_stopping_rounds': 100,
    'grow_policy': 'Lossguide',
    'min_data_in_leaf': 10,
    'max_leaves': 64
}

"""# IMPROVED MODEL EVALUATION"""

def evaluate_model_improved(model, X, y, nombre_modelo, grupo_familia=None, grupo_tienda=None):
    """
    Improved model evaluation with more comprehensive metrics
    """
    print(f"\n--- IMPROVED RESULTS for {nombre_modelo} ---")
    
    # Get predictions
    y_pred = model.predict(X)
    
    # Calculate metrics
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    mae = mean_absolute_error(y, y_pred)
    
    print(f"R¬≤ Score: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"Mean Target: {y.mean():.2f}")
    print(f"Std Target: {y.std():.2f}")
    
    # Feature importance
    if hasattr(model, 'get_feature_importance'):
        importancias = model.get_feature_importance()
        feature_importance_df = pd.DataFrame({
            'Feature': X.columns,
            'Importance': importancias
        }).sort_values(by='Importance', ascending=False)
        
        print("\nTop 10 Most Important Features:")
        print(feature_importance_df.head(10))
    
    # Detailed analysis by groups
    if grupo_familia is not None:
        print("\n--- Metrics by Family ---")
        df_fam = pd.DataFrame({'Real': y, 'Predicci√≥n': y_pred, 'Familia': grupo_familia})
        resumen_fam = df_fam.groupby('Familia').apply(
            lambda x: pd.Series({
                'R2': r2_score(x['Real'], x['Predicci√≥n']),
                'RMSE': np.sqrt(mean_squared_error(x['Real'], x['Predicci√≥n'])),
                'MAE': mean_absolute_error(x['Real'], x['Predicci√≥n']),
                'N': len(x),
                'Mean_Real': x['Real'].mean(),
                'Mean_Pred': x['Predicci√≥n'].mean()
            })
        ).sort_values(by='RMSE', ascending=False)
        print(resumen_fam)
    
    if grupo_tienda is not None:
        print("\n--- Metrics by Store ---")
        df_tienda = pd.DataFrame({'Real': y, 'Predicci√≥n': y_pred, 'Tienda': grupo_tienda})
        resumen_tienda = df_tienda.groupby('Tienda').apply(
            lambda x: pd.Series({
                'R2': r2_score(x['Real'], x['Predicci√≥n']),
                'RMSE': np.sqrt(mean_squared_error(x['Real'], x['Predicci√≥n'])),
                'MAE': mean_absolute_error(x['Real'], x['Predicci√≥n']),
                'N': len(x),
                'Mean_Real': x['Real'].mean(),
                'Mean_Pred': x['Predicci√≥n'].mean()
            })
        ).sort_values(by='RMSE', ascending=False)
        print(resumen_tienda)
    
    return r2, rmse, mae

"""# IMPROVED PREDICTION FUNCTION"""

def predict_future_sales_improved(monthly, modelos_en, model_general_en, modelos_fuera, model_general_fuera, 
                                 meses_futuros, sanitizar_nombre=None):
    """
    Improved prediction function with better error handling and validation
    """
    monthly = monthly.copy()
    meses_futuros = sorted(pd.to_datetime(meses_futuros))
    
    # Initialize prediction columns
    monthly['Pred_Cantidad_en_temporada'] = np.nan
    monthly['Pred_Cantidad_fuera_temporada'] = np.nan
    
    # Group by store and family for more granular predictions
    group_cols = ['Tienda', 'Descripci√≥n Familia']
    
    for key, df_grupo in monthly.groupby(group_cols):
        df_grupo = df_grupo.sort_values('ds').copy()
        
        # Predict for each target
        for target_col, pred_col, modelos, model_general in [
            ('Cantidad_en_temporada', 'Pred_Cantidad_en_temporada', modelos_en, model_general_en),
            ('Cantidad_fuera_temporada', 'Pred_Cantidad_fuera_temporada', modelos_fuera, model_general_fuera)
        ]:
            
            # Check if we have enough historical data
            historical_data = df_grupo[df_grupo[target_col] > 0]
            if len(historical_data) < 12:  # Need at least 12 months of data
                continue
            
            # Predict for future months
            for mes in meses_futuros:
                idx = (df_grupo['ds'] == mes)
                if not idx.any():
                    continue
                
                row = df_grupo.loc[idx].iloc[0]
                familia = row['Descripci√≥n Familia']
                tienda = row['Tienda']
                
                # Choose model (specific family model or general model)
                fam_sanit = sanitizar_nombre(familia) if sanitizar_nombre else familia
                
                if fam_sanit in modelos:
                    modelo = modelos[fam_sanit]['model']
                    features = modelos[fam_sanit]['features']
                    cat_feats = modelos[fam_sanit]['cat_features']
                else:
                    modelo = model_general['model']
                    features = model_general['features']
                    cat_feats = model_general['cat_features']
                
                # Prepare input data
                X = pd.DataFrame([{f: row.get(f, np.nan) for f in features}])
                
                # Handle missing values with historical averages
                for col in X.columns:
                    if X[col].isna().any():
                        if 'lag' in col or 'rolling' in col or 'tendencia' in col:
                            # Use historical average for lag features
                            hist_avg = df_grupo[col].dropna().mean()
                            X[col] = X[col].fillna(hist_avg if not np.isnan(hist_avg) else 0)
                        else:
                            # Use 0 for other features
                            X[col] = X[col].fillna(0)
                
                # Ensure categorical features are strings
                for col in cat_feats:
                    if col in X.columns:
                        X[col] = X[col].astype(str).fillna('missing')
                
                # Make prediction
                try:
                    if hasattr(modelo, 'predict') and 'catboost' in str(type(modelo)).lower():
                        cat_indices = [features.index(c) for c in cat_feats if c in features]
                        pool = Pool(X, cat_features=cat_indices)
                        pred = modelo.predict(pool)
                    else:
                        pred = modelo.predict(X)
                    
                    pred_val = max(0, float(pred[0]))  # Ensure non-negative predictions
                    
                    # Update prediction in main dataframe
                    monthly.loc[
                        (monthly['Tienda'] == tienda) &
                        (monthly['Descripci√≥n Familia'] == familia) &
                        (monthly['ds'] == mes),
                        pred_col
                    ] = pred_val
                    
                except Exception as e:
                    print(f"Error predicting for {tienda} - {familia} - {mes}: {e}")
                    continue
    
    return monthly

# Example usage (you'll need to implement the model loading part)
# monthly_pred_improved = predict_future_sales_improved(
#     monthly, modelos_en_temporada, model_general_en, 
#     modelos_fuera_temporada, model_general_fuera,
#     meses_futuros, sanitizar_nombre=sanitizar_nombre
# )

"""# ADDITIONAL IMPROVEMENTS"""

# 1. ENSEMBLE APPROACH
def create_ensemble_model(X, y, categorical_features):
    """
    Create an ensemble of different models for better predictions
    """
    models = {}
    
    # CatBoost
    cat_indices = [X.columns.get_loc(c) for c in categorical_features if c in X.columns]
    pool = Pool(X, y, cat_features=cat_indices)
    catboost_model = CatBoostRegressor(
        iterations=1000, learning_rate=0.01, depth=6, 
        loss_function='RMSE', random_seed=42, verbose=0
    )
    catboost_model.fit(pool)
    models['catboost'] = catboost_model
    
    # XGBoost (with proper categorical handling)
    xgb_model = XGBRegressor(
        n_estimators=1000, learning_rate=0.01, max_depth=6,
        random_state=42, n_jobs=-1
    )
    xgb_model.fit(X, y)
    models['xgboost'] = xgb_model
    
    # Random Forest
    rf_model = RandomForestRegressor(
        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
    )
    rf_model.fit(X, y)
    models['random_forest'] = rf_model
    
    return models

def ensemble_predict(models, X, categorical_features):
    """
    Make ensemble predictions
    """
    predictions = {}
    
    # CatBoost prediction
    cat_indices = [X.columns.get_loc(c) for c in categorical_features if c in X.columns]
    pool = Pool(X, cat_features=cat_indices)
    predictions['catboost'] = models['catboost'].predict(pool)
    
    # XGBoost prediction
    predictions['xgboost'] = models['xgboost'].predict(X)
    
    # Random Forest prediction
    predictions['random_forest'] = models['random_forest'].predict(X)
    
    # Weighted average (you can tune these weights)
    weights = {'catboost': 0.5, 'xgboost': 0.3, 'random_forest': 0.2}
    ensemble_pred = sum(predictions[model] * weight for model, weight in weights.items())
    
    return ensemble_pred

# 2. CROSS-VALIDATION FOR TIME SERIES
def time_series_cv_evaluation(X, y, categorical_features, n_splits=5):
    """
    Perform time series cross-validation
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # Train model
        cat_indices = [X_train.columns.get_loc(c) for c in categorical_features if c in X_train.columns]
        pool = Pool(X_train, y_train, cat_features=cat_indices)
        
        model = CatBoostRegressor(
            iterations=500, learning_rate=0.01, depth=6,
            loss_function='RMSE', random_seed=42, verbose=0
        )
        model.fit(pool)
        
        # Predict
        val_pool = Pool(X_val, cat_features=cat_indices)
        y_pred = model.predict(val_pool)
        
        # Calculate score
        score = r2_score(y_val, y_pred)
        scores.append(score)
    
    return np.mean(scores), np.std(scores)

# Example usage:
# mean_score, std_score = time_series_cv_evaluation(X_en, y_en, cat_features_en)
# print(f"Cross-validation R¬≤: {mean_score:.4f} ¬± {std_score:.4f}")

"""# SAVE IMPROVED MODELS"""

import joblib
import json
import os

"""# FINAL IMPROVEMENTS AND VALIDATION"""

# 1. PREVENT DATA LEAKAGE
def check_data_leakage(df, target_col, date_col='ds'):
    """
    Check for potential data leakage in the dataset
    """
    print("üîç Checking for data leakage...")
    
    # Check if future data is being used to predict past
    df_sorted = df.sort_values(date_col)
    leakage_found = False
    
    for i in range(len(df_sorted) - 1):
        current_date = df_sorted.iloc[i][date_col]
        next_date = df_sorted.iloc[i + 1][date_col]
        
        if current_date > next_date:
            print(f"‚ö†Ô∏è Potential data leakage: {current_date} > {next_date}")
            leakage_found = True
    
    if not leakage_found:
        print("‚úÖ No obvious data leakage detected")
    
    return not leakage_found

# 2. IMPROVED FEATURE SELECTION
def select_best_features(X, y, categorical_features, max_features=50):
    """
    Use feature importance to select the best features
    """
    print("üîç Selecting best features...")
    
    # Train a quick model to get feature importance
    cat_indices = [X.columns.get_loc(c) for c in categorical_features if c in X.columns]
    pool = Pool(X, y, cat_features=cat_indices)
    
    model = CatBoostRegressor(
        iterations=500, learning_rate=0.01, depth=6,
        loss_function='RMSE', random_seed=42, verbose=0
    )
    model.fit(pool)
    
    # Get feature importance
    importances = model.get_feature_importance()
    feature_importance_df = pd.DataFrame({
        'Feature': X.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    # Select top features
    top_features = feature_importance_df.head(max_features)['Feature'].tolist()
    
    # Always include categorical features
    for cat_feat in categorical_features:
        if cat_feat not in top_features:
            top_features.append(cat_feat)
    
    print(f"‚úÖ Selected {len(top_features)} features out of {len(X.columns)}")
    print("Top 10 features:")
    print(feature_importance_df.head(10))
    
    return top_features

# 3. ROBUST PREDICTION VALIDATION
def validate_predictions(real_values, predicted_values, tolerance=0.2):
    """
    Validate predictions with multiple criteria
    """
    print("üîç Validating predictions...")
    
    # Basic metrics
    r2 = r2_score(real_values, predicted_values)
    rmse = np.sqrt(mean_squared_error(real_values, predicted_values))
    mae = mean_absolute_error(real_values, predicted_values)
    
    # Additional validation metrics
    mape = np.mean(np.abs((real_values - predicted_values) / (real_values + 1e-8))) * 100
    
    # Check for reasonable predictions
    negative_predictions = np.sum(predicted_values < 0)
    extreme_predictions = np.sum(predicted_values > real_values.max() * 3)
    
    # Direction accuracy (trend prediction)
    real_trend = np.diff(real_values)
    pred_trend = np.diff(predicted_values)
    direction_accuracy = np.mean((real_trend > 0) == (pred_trend > 0))
    
    print(f"R¬≤ Score: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"MAPE: {mape:.2f}%")
    print(f"Negative predictions: {negative_predictions}")
    print(f"Extreme predictions (>3x max): {extreme_predictions}")
    print(f"Direction accuracy: {direction_accuracy:.4f}")
    
    # Validation flags
    warnings = []
    if r2 < 0.5:
        warnings.append("Low R¬≤ score - model may not be capturing patterns well")
    if mape > 50:
        warnings.append("High MAPE - predictions may be too inaccurate")
    if negative_predictions > 0:
        warnings.append("Negative predictions found - consider constraints")
    if extreme_predictions > 0:
        warnings.append("Extreme predictions found - check for outliers")
    
    if warnings:
        print("‚ö†Ô∏è Validation warnings:")
        for warning in warnings:
            print(f"  - {warning}")
    else:
        print("‚úÖ All validation checks passed")
    
    return {
        'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape,
        'direction_accuracy': direction_accuracy,
        'warnings': warnings
    }

# 4. IMPROVED MODEL COMPARISON
def compare_models(models_dict, X, y, categorical_features):
    """
    Compare multiple models and select the best one
    """
    print("üîç Comparing models...")
    
    results = {}
    
    for name, model in models_dict.items():
        print(f"\nTesting {name}...")
        
        try:
            if 'catboost' in name.lower():
                cat_indices = [X.columns.get_loc(c) for c in categorical_features if c in X.columns]
                pool = Pool(X, cat_features=cat_indices)
                y_pred = model.predict(pool)
            else:
                y_pred = model.predict(X)
            
            # Calculate metrics
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))
            mae = mean_absolute_error(y, y_pred)
            
            results[name] = {
                'r2': r2, 'rmse': rmse, 'mae': mae,
                'predictions': y_pred
            }
            
            print(f"  R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")
            
        except Exception as e:
            print(f"  Error with {name}: {e}")
            results[name] = None
    
    # Find best model
    valid_results = {k: v for k, v in results.items() if v is not None}
    if valid_results:
        best_model = min(valid_results.keys(), key=lambda x: valid_results[x]['rmse'])
        print(f"\nüèÜ Best model: {best_model}")
        return best_model, results
    else:
        print("‚ùå No valid models found")
        return None, results

# 5. FINAL TRAINING FUNCTION WITH ALL IMPROVEMENTS
def train_final_model(X, y, categorical_features, target_name):
    """
    Final training function with all improvements
    """
    print(f"\nüöÄ Training final model for {target_name}")
    
    # Check data leakage (skip for now since we don't have date column)
    # check_data_leakage(pd.DataFrame({'target': y, 'index': range(len(y))}), 'target')
    
    # Select best features
    best_features = select_best_features(X, y, categorical_features)
    X_selected = X[best_features]
    
    # Update categorical features for selected features
    selected_cat_features = [f for f in categorical_features if f in best_features]
    
    # Create multiple models
    models = {}
    
    # CatBoost with different configurations
    cat_indices = [X_selected.columns.get_loc(c) for c in selected_cat_features if c in X_selected.columns]
    pool = Pool(X_selected, y, cat_features=cat_indices)
    
    # Model 1: Conservative
    model1 = CatBoostRegressor(
        iterations=1000, learning_rate=0.005, depth=6,
        l2_leaf_reg=5, loss_function='RMSE', random_seed=42, verbose=0
    )
    model1.fit(pool)
    models['catboost_conservative'] = model1
    
    # Model 2: Aggressive
    model2 = CatBoostRegressor(
        iterations=1500, learning_rate=0.01, depth=8,
        l2_leaf_reg=3, loss_function='RMSE', random_seed=42, verbose=0
    )
    model2.fit(pool)
    models['catboost_aggressive'] = model2
    
    # Model 3: Balanced
    model3 = CatBoostRegressor(
        iterations=1200, learning_rate=0.008, depth=7,
        l2_leaf_reg=4, loss_function='RMSE', random_seed=42, verbose=0
    )
    model3.fit(pool)
    models['catboost_balanced'] = model3
    
    # Compare models
    best_model_name, all_results = compare_models(models, X_selected, y, selected_cat_features)
    
    if best_model_name:
        best_model = models[best_model_name]
        
        # Validate predictions
        if best_model_name.startswith('catboost'):
            pool_val = Pool(X_selected, cat_features=cat_indices)
            y_pred = best_model.predict(pool_val)
        else:
            y_pred = best_model.predict(X_selected)
        
        validation_results = validate_predictions(y, y_pred)
        
        return {
            'model': best_model,
            'features': best_features,
            'categorical_features': selected_cat_features,
            'model_name': best_model_name,
            'validation': validation_results,
            'all_results': all_results
        }
    else:
        return None

# Model functions are now ready to be imported and used

# IMPROVED MODEL TRAINING WITH OVERFITTING PREVENTION

def train_robust_model(X, y, categorical_features, target_name, validation_split=0.2):
    """
    Train a robust model with proper validation to prevent overfitting
    """
    print(f"\nüöÄ Training robust model for {target_name}")
    
    # 1. TIME SERIES SPLIT (CRITICAL FOR PREVENTING DATA LEAKAGE)
    from sklearn.model_selection import TimeSeriesSplit
    
    # Sort by date to ensure proper time series split
    if 'a√±o' in X.columns and 'mes' in X.columns:
        X['date_sort'] = X['a√±o'] * 100 + X['mes']
        X_sorted = X.sort_values('date_sort')
        y_sorted = y[X_sorted.index]
        X_sorted = X_sorted.drop('date_sort', axis=1)
    else:
        X_sorted = X
        y_sorted = y
    
    # Use time series cross-validation
    tscv = TimeSeriesSplit(n_splits=3)
    
    # 2. FEATURE SELECTION WITH VALIDATION
    print("üîç Selecting features with validation...")
    
    # Get feature importance from a quick model
    cat_indices = [X_sorted.columns.get_loc(c) for c in categorical_features if c in X_sorted.columns]
    pool = Pool(X_sorted, y_sorted, cat_features=cat_indices)
    
    quick_model = CatBoostRegressor(
        iterations=200, learning_rate=0.1, depth=4,
        loss_function='RMSE', random_seed=42, verbose=0
    )
    quick_model.fit(pool)
    
    # Get feature importance
    importances = quick_model.get_feature_importance()
    feature_importance_df = pd.DataFrame({
        'Feature': X_sorted.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    # Select top features (reduce complexity to prevent overfitting)
    max_features = min(30, len(X_sorted.columns))
    top_features = feature_importance_df.head(max_features)['Feature'].tolist()
    
    # Always include categorical features
    for cat_feat in categorical_features:
        if cat_feat not in top_features:
            top_features.append(cat_feat)
    
    print(f"‚úÖ Selected {len(top_features)} features out of {len(X_sorted.columns)}")
    print("Top 10 features:")
    print(feature_importance_df.head(10))
    
    # 3. CROSS-VALIDATION WITH MULTIPLE MODELS
    print("üîç Cross-validating multiple models...")
    
    X_selected = X_sorted[top_features]
    selected_cat_features = [f for f in categorical_features if f in top_features]
    
    models = {}
    cv_scores = {}
    
    # Model configurations to test
    model_configs = {
        'conservative': {
            'iterations': 500, 'learning_rate': 0.01, 'depth': 4,
            'l2_leaf_reg': 10, 'subsample': 0.8, 'colsample_bylevel': 0.8
        },
        'balanced': {
            'iterations': 800, 'learning_rate': 0.02, 'depth': 6,
            'l2_leaf_reg': 5, 'subsample': 0.85, 'colsample_bylevel': 0.85
        },
        'aggressive': {
            'iterations': 1000, 'learning_rate': 0.03, 'depth': 8,
            'l2_leaf_reg': 3, 'subsample': 0.9, 'colsample_bylevel': 0.9
        }
    }
    
    for name, config in model_configs.items():
        print(f"\nTesting {name} configuration...")
        
        scores = []
        for train_idx, val_idx in tscv.split(X_selected):
            X_train, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
            y_train, y_val = y_sorted.iloc[train_idx], y_sorted.iloc[val_idx]
            
            # Train model
            cat_indices = [X_train.columns.get_loc(c) for c in selected_cat_features if c in X_train.columns]
            pool_train = Pool(X_train, y_train, cat_features=cat_indices)
            
            model = CatBoostRegressor(
                **config,
                loss_function='RMSE',
                random_seed=42,
                verbose=0,
                early_stopping_rounds=50
            )
            
            # Use validation set for early stopping
            pool_val = Pool(X_val, y_val, cat_features=cat_indices)
            model.fit(pool_train, eval_set=pool_val, verbose=0)
            
            # Predict
            y_pred = model.predict(pool_val)
            
            # Calculate metrics
            r2 = r2_score(y_val, y_pred)
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            mae = mean_absolute_error(y_val, y_pred)
            
            scores.append({'r2': r2, 'rmse': rmse, 'mae': mae})
        
        # Average scores
        avg_r2 = np.mean([s['r2'] for s in scores])
        avg_rmse = np.mean([s['rmse'] for s in scores])
        avg_mae = np.mean([s['mae'] for s in scores])
        
        cv_scores[name] = {
            'r2': avg_r2, 'rmse': avg_rmse, 'mae': avg_mae,
            'std_r2': np.std([s['r2'] for s in scores]),
            'std_rmse': np.std([s['rmse'] for s in scores])
        }
        
        print(f"  CV R¬≤: {avg_r2:.4f} ¬± {cv_scores[name]['std_r2']:.4f}")
        print(f"  CV RMSE: {avg_rmse:.4f} ¬± {cv_scores[name]['std_rmse']:.4f}")
    
    # 4. SELECT BEST MODEL
    best_model_name = min(cv_scores.keys(), key=lambda x: cv_scores[x]['rmse'])
    best_config = model_configs[best_model_name]
    
    print(f"\nüèÜ Best model: {best_model_name}")
    print(f"  R¬≤: {cv_scores[best_model_name]['r2']:.4f}")
    print(f"  RMSE: {cv_scores[best_model_name]['rmse']:.4f}")
    
    # 5. TRAIN FINAL MODEL ON FULL DATA
    print("üîß Training final model on full dataset...")
    
    cat_indices = [X_selected.columns.get_loc(c) for c in selected_cat_features if c in X_selected.columns]
    pool_full = Pool(X_selected, y_sorted, cat_features=cat_indices)
    
    final_model = CatBoostRegressor(
        **best_config,
        loss_function='RMSE',
        random_seed=42,
        verbose=100
    )
    
    final_model.fit(pool_full)
    
    # 6. VALIDATE FINAL MODEL
    print("üîç Validating final model...")
    
    y_pred_full = final_model.predict(pool_full)
    
    # Calculate final metrics
    final_r2 = r2_score(y_sorted, y_pred_full)
    final_rmse = np.sqrt(mean_squared_error(y_sorted, y_pred_full))
    final_mae = mean_absolute_error(y_sorted, y_pred_full)
    
    # Check for overfitting (CV score vs full data score)
    cv_r2 = cv_scores[best_model_name]['r2']
    overfitting_score = cv_r2 - final_r2
    
    print(f"Final R¬≤: {final_r2:.4f}")
    print(f"CV R¬≤: {cv_r2:.4f}")
    print(f"Overfitting indicator: {overfitting_score:.4f}")
    
    if overfitting_score > 0.1:
        print("‚ö†Ô∏è Potential overfitting detected! Consider simpler model.")
    
    # 7. PREDICTION CONSTRAINTS
    print("üîí Adding prediction constraints...")
    
    # Ensure predictions are non-negative
    y_pred_constrained = np.maximum(y_pred_full, 0)
    
    # Cap extreme predictions (no more than 3x the maximum observed value)
    max_observed = y_sorted.max()
    y_pred_constrained = np.minimum(y_pred_constrained, max_observed * 3)
    
    # Recalculate metrics with constraints
    final_r2_constrained = r2_score(y_sorted, y_pred_constrained)
    final_rmse_constrained = np.sqrt(mean_squared_error(y_sorted, y_pred_constrained))
    
    print(f"Constrained R¬≤: {final_r2_constrained:.4f}")
    print(f"Constrained RMSE: {final_rmse_constrained:.4f}")
    
    return {
        'model': final_model,
        'features': top_features,
        'categorical_features': selected_cat_features,
        'model_name': f"catboost_{best_model_name}",
        'validation': {
            'r2': final_r2_constrained,
            'rmse': final_rmse_constrained,
            'mae': final_mae,
            'cv_r2': cv_r2,
            'cv_rmse': cv_scores[best_model_name]['rmse'],
            'overfitting_score': overfitting_score,
            'cv_scores': cv_scores
        },
        'prediction_constraints': {
            'min_value': 0,
            'max_multiplier': 3
        }
    }

def predict_with_constraints(model_info, X_new):
    """
    Make predictions with proper constraints
    """
    model = model_info['model']
    features = model_info['features']
    cat_features = model_info['categorical_features']
    constraints = model_info['prediction_constraints']
    
    # Prepare data
    X_prepared = X_new[features].fillna(0)
    cat_indices = [X_prepared.columns.get_loc(c) for c in cat_features if c in X_prepared.columns]
    pool = Pool(X_prepared, cat_features=cat_indices)
    
    # Make predictions
    predictions = model.predict(pool)
    
    # Apply constraints
    predictions = np.maximum(predictions, constraints['min_value'])
    
    # Cap extreme predictions
    if 'max_multiplier' in constraints:
        max_observed = X_new.get('Cantidad', 0).max() if 'Cantidad' in X_new.columns else 1000
        predictions = np.minimum(predictions, max_observed * constraints['max_multiplier'])
    
    return predictions

# IMPROVED DATA PREPARATION
def prepare_data_with_validation(df, meses_futuros=None):
    """
    Prepare data with validation checks to prevent data leakage
    """
    print("üîç Preparing data with validation...")
    
    # Use the existing improved function
    en_temporada, fuera_temporada, monthly = prepare_final_dataset_improved(df, meses_futuros)
    
    # Add validation checks
    print("üîç Checking data quality...")
    
    # Check for data leakage in time series
    if 'ds' in monthly.columns:
        monthly_sorted = monthly.sort_values('ds')
        date_issues = 0
        for i in range(len(monthly_sorted) - 1):
            if monthly_sorted.iloc[i]['ds'] > monthly_sorted.iloc[i + 1]['ds']:
                date_issues += 1
        
        if date_issues > 0:
            print(f"‚ö†Ô∏è Found {date_issues} potential date ordering issues")
        else:
            print("‚úÖ Date ordering is correct")
    
    # Check for extreme values
    for col in ['Cantidad_en_temporada', 'Cantidad_fuera_temporada']:
        if col in monthly.columns:
            q99 = monthly[col].quantile(0.99)
            extreme_count = (monthly[col] > q99 * 3).sum()
            if extreme_count > 0:
                print(f"‚ö†Ô∏è Found {extreme_count} extreme values in {col}")
    
    # Check for missing values
    missing_counts = monthly.isnull().sum()
    if missing_counts.sum() > 0:
        print("‚ö†Ô∏è Missing values found:")
        print(missing_counts[missing_counts > 0])
    
    return en_temporada, fuera_temporada, monthly